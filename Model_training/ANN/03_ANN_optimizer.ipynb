{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09239269",
   "metadata": {},
   "source": [
    "<h2 style=\"color: green;\">3. 옵티마이저</h2>\n",
    "\n",
    "옵티마이저(Optimizer)란, 딥러닝 모델의 학습 과정에서 모델의 파라미터를 업데이트하는 알고리즘입니다. 즉, 모델이 학습 데이터에 대해 최적의 결과를 도출하기 위해 모델의 가중치(weight)와 편향(bias)을 조정하는 데 사용됩니다.\n",
    "\n",
    "옵티마이저는 손실 함수(loss function)에서 계산된 그래디언트(gradient)를 이용하여 모델의 파라미터를 업데이트합니다. 손실 함수는 모델의 예측 값과 실제 값의 차이를 계산하는 함수로, 이 값을 최소화하는 방향으로 모델의 파라미터를 조정합니다.\n",
    "\n",
    "**옵티마이저의 종류**\n",
    "\n",
    "+ 경사 하강법(Gradient Descent)\n",
    "<br></br>\n",
    "+ 모멘텀(Momentum)\n",
    "<br></br>\n",
    "+ 아다그라드(Adagrad)\n",
    "<br></br>\n",
    "+ 알엠에스프롭(RMSprop)\n",
    "<br></br>\n",
    "+ 아담(Adam)\n",
    "\n",
    "<br></br>\n",
    "## 3-1. Gradient Descent Optimization\n",
    "\n",
    "가장 기본적인 최적화 알고리즘 중 하나로, 가중치를 조정할 때 매개변수에 대한 손실함수의 기울기 (gradient)를 이용하여 최적화하는 방법입니다. 즉, 가중치 업데이트는 현재 가중치에서 기울기를 빼는 방식으로 이루어집니다.\n",
    "\n",
    "Gradient Descent는 크게 Batch Gradient Descent, Stochastic Gradient Descent(SGD), Mini-batch Gradient Descent로 나뉩니다.\n",
    "<br></br>\n",
    "\n",
    "+ ### *Stochastic Gradient Descent (SGD)*\n",
    "\n",
    "Stochastic Gradient Descent는 데이터 하나씩 기울기를 계산하고 가중치를 업데이트합니다. 즉, 데이터 한 개씩 처리하므로 전체 데이터셋보다 메모리 사용량이 적지만, 최소값에 수렴하는 속도가 느릴 수 있습니다. 또한, 경사 하강 방향이 매번 달라져서 최적값에 수렴하는 과정에서 지그재그로 이동하는 현상이 발생할 수 있습니다.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf283c6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
