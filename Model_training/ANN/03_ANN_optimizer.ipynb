{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09239269",
   "metadata": {},
   "source": [
    "<h2 style=\"color: green;\">3. 옵티마이저</h2>\n",
    "\n",
    "옵티마이저(Optimizer)란, 딥러닝 모델의 학습 과정에서 모델의 파라미터를 업데이트하는 알고리즘입니다. 즉, 모델이 학습 데이터에 대해 최적의 결과를 도출하기 위해 모델의 가중치(weight)와 편향(bias)을 조정하는 데 사용됩니다.\n",
    "\n",
    "옵티마이저는 손실 함수(loss function)에서 계산된 그래디언트(gradient)를 이용하여 모델의 파라미터를 업데이트합니다. 손실 함수는 모델의 예측 값과 실제 값의 차이를 계산하는 함수로, 이 값을 최소화하는 방향으로 모델의 파라미터를 조정합니다.\n",
    "\n",
    "**옵티마이저의 종류**\n",
    "\n",
    "+ 경사 하강법(Gradient Descent)\n",
    "<br></br>\n",
    "+ 모멘텀(Momentum)\n",
    "<br></br>\n",
    "+ 아다그라드(Adagrad)\n",
    "<br></br>\n",
    "+ 알엠에스프롭(RMSprop)\n",
    "<br></br>\n",
    "+ 아담(Adam)\n",
    "\n",
    "<br></br>\n",
    "## 3-1. 경사 하강법 (Gradient Descent Optimization)\n",
    "\n",
    "가장 기본적인 최적화 알고리즘 중 하나로, 가중치를 조정할 때 매개변수에 대한 손실함수의 기울기 (gradient)를 이용하여 최적화하는 방법입니다. 즉, 가중치 업데이트는 현재 가중치에서 기울기를 빼는 방식으로 이루어집니다.\n",
    "\n",
    "Gradient Descent는 크게 Batch Gradient Descent, Stochastic Gradient Descent(SGD), Mini-batch Gradient Descent로 나뉩니다.\n",
    "<br></br>\n",
    "\n",
    "+ ### 배치 경사 하강법*Batch Gradient Descent (SGD)*\n",
    "\n",
    "Batch Gradient Descent는 전체 데이터셋에 대해 기울기를 계산하고 가중치를 업데이트합니다. 즉, 전체 데이터셋을 한번에 처리하므로 한번에 메모리를 많이 사용하게 되고, 처리시간이 오래 걸릴 수 있습니다. 하지만 전체 데이터셋에 대한 기울기를 계산하므로 최소값에 수렴하는 속도는 빠릅니다.\n",
    "\n",
    "+ ### 미니배치 경사 하강법*Mini-batch Gradient Descent*\n",
    "\n",
    "Mini-batch Gradient Descent는 전체 데이터셋의 일부(mini-batch)에 대해서만 기울기를 계산하고 가중치를 업데이트합니다. 즉, 전체 데이터셋보다 메모리 사용량이 적으면서, 한 개의 데이터를 처리 하는 SGD보다 안정적으로 최적값에 수렴할 수 있습니다.\n",
    "\n",
    "+ ### 확률적 경사 하강법*Stochastic Gradient Descent (SGD)*\n",
    "\n",
    "Stochastic Gradient Descent는 데이터 하나씩 기울기를 계산하고 가중치를 업데이트합니다. 즉, 데이터 한 개씩 처리하므로 전체 데이터셋보다 메모리 사용량이 적지만, 최소값에 수렴하는 속도가 느릴 수 있습니다. 또한, 경사 하강 방향이 매번 달라져서 최적값에 수렴하는 과정에서 지그재그로 이동하는 현상이 발생할 수 있습니다.\n",
    "\n",
    "+ ### 모멘텀 최적화 *Momentum Optimization*\n",
    "\n",
    "Momentum Optimization은 Gradient Descent의 한계를 보완하기 위해 등장한 방법 중 하나입니다. 이전 기울기의 방향과 크기를 고려하여 새로운 기울기를 계산합니다. 이전 기울기의 방향이 현재 기울기와 일치하면 가중치를 더 크게 업데이트하고, 그렇지 않으면 더 작게 업데이트합니다.\n",
    "\n",
    "예를 들어, 기울기가 대각선 방향으로 지속적으로 나오는 경우, 일반적인 Gradient Descent는 오랜 시간 동안 최적점에 수렴하지 못하고 지그재그로 움직이게 됩니다. 하지만 Momentum Optimization은 이전 기울기를 더해서 가중치 업데이트를 수행하기 때문에 이전 방향을 유지하면서 최적점에 빠르게 수렴할 수 있습니다.\n",
    "\n",
    "이전 기울기와 현재 기울기의 비율을 조절하는 momentum 하이퍼파라미터를 설정할 수 있습니다. momentum 값이 0에 가까울수록 Gradient Descent와 유사해지며, 1에 가까울수록 이전 방향을 보존하는 정도가 높아집니다. 적절한 momentum 값을 설정하면 보다 빠르고 안정적인 학습을 할 수 있습니다.\n",
    "\n",
    "+ ### *Adam*\n",
    "\n",
    "Momentum Optimization과 Adagrad Optimization의 아이디어를 결합한 옵티마이저입니다. Adam은 각 매개 변수마다 적응적인 학습률을 사용하며, 이전 기울기의 지수 가중 이동 평균과 이전 기울기 제곱의 지수 가중 이동 평균을 계산하여 학습률을 조정합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5961c2f2",
   "metadata": {},
   "source": [
    "## *선형 회귀 모델의 학습에서 다양한 옵티마이저를 적용*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "039656f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0d4101f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "california = fetch_california_housing()\n",
    "x = california.data\n",
    "y = california.target\n",
    "\n",
    "# 데이터 스케일링\n",
    "scaler = StandardScaler()\n",
    "x = scaler.fit_transform(x)\n",
    "# 데이터를 표준화(standardization)합니다. 표준화는 각 특징의 평균을 0, 표준 편차를 1로 만들어 데이터의 스케일을 일정하게 조정하는 과정입니다.\n",
    "\n",
    "# 데이터 분할\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 모델 생성 및 하이퍼파라미터 설정\n",
    "input_dim = x.shape[1]\n",
    "output_dim = 1\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 1000\n",
    "\n",
    "model = nn.Linear(input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b977c8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#다양한 옵티마이저 설정\n",
    "optimizers = {\"SGD\" : optim.SGD(model.parameters(), lr=learning_rate),\n",
    "             \"Momentum\" : optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9),\n",
    "             \"Adagrad\" : optim.Adagrad(model.parameters(), lr=learning_rate),\n",
    "             \"RMSprop\" : optim.RMSprop(model.parameters(), lr=learning_rate),\n",
    "             \"Adam\" : optim.Adam(model.parameters(), lr=learning_rate)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00ca5c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD - Epoch [100/1000], Loss : 0.7629\n",
      "SGD - Epoch [200/1000], Loss : 4.9919\n",
      "SGD - Epoch [300/1000], Loss : 1.6641\n",
      "SGD - Epoch [400/1000], Loss : 3.6604\n",
      "SGD - Epoch [500/1000], Loss : 3.0820\n",
      "SGD - Epoch [600/1000], Loss : 2.2910\n",
      "SGD - Epoch [700/1000], Loss : 4.4701\n",
      "SGD - Epoch [800/1000], Loss : 1.2289\n",
      "SGD - Epoch [900/1000], Loss : 5.3093\n",
      "SGD - Epoch [1000/1000], Loss : 0.6956\n",
      "Momentum - Epoch [100/1000], Loss : 0.7114\n",
      "Momentum - Epoch [200/1000], Loss : 1.8788\n",
      "Momentum - Epoch [300/1000], Loss : 54.5746\n",
      "Momentum - Epoch [400/1000], Loss : 333.6345\n",
      "Momentum - Epoch [500/1000], Loss : 741.2551\n",
      "Momentum - Epoch [600/1000], Loss : 29782.7305\n",
      "Momentum - Epoch [700/1000], Loss : 298174.6562\n",
      "Momentum - Epoch [800/1000], Loss : 943067.5000\n",
      "Momentum - Epoch [900/1000], Loss : 22727518.0000\n",
      "Momentum - Epoch [1000/1000], Loss : 299659776.0000\n",
      "Adagrad - Epoch [100/1000], Loss : 314379136.0000\n",
      "Adagrad - Epoch [200/1000], Loss : 314379136.0000\n",
      "Adagrad - Epoch [300/1000], Loss : 314379136.0000\n",
      "Adagrad - Epoch [400/1000], Loss : 314379136.0000\n",
      "Adagrad - Epoch [500/1000], Loss : 314379136.0000\n",
      "Adagrad - Epoch [600/1000], Loss : 314379136.0000\n",
      "Adagrad - Epoch [700/1000], Loss : 314379136.0000\n",
      "Adagrad - Epoch [800/1000], Loss : 314379136.0000\n",
      "Adagrad - Epoch [900/1000], Loss : 314379136.0000\n",
      "Adagrad - Epoch [1000/1000], Loss : 314379136.0000\n",
      "RMSprop - Epoch [100/1000], Loss : 314375616.0000\n",
      "RMSprop - Epoch [200/1000], Loss : 314374912.0000\n",
      "RMSprop - Epoch [300/1000], Loss : 314374304.0000\n",
      "RMSprop - Epoch [400/1000], Loss : 314373600.0000\n",
      "RMSprop - Epoch [500/1000], Loss : 314373056.0000\n",
      "RMSprop - Epoch [600/1000], Loss : 314372992.0000\n",
      "RMSprop - Epoch [700/1000], Loss : 314372928.0000\n",
      "RMSprop - Epoch [800/1000], Loss : 314372864.0000\n",
      "RMSprop - Epoch [900/1000], Loss : 314372736.0000\n",
      "RMSprop - Epoch [1000/1000], Loss : 314372640.0000\n",
      "Adam - Epoch [100/1000], Loss : 314372128.0000\n",
      "Adam - Epoch [200/1000], Loss : 314371488.0000\n",
      "Adam - Epoch [300/1000], Loss : 314370816.0000\n",
      "Adam - Epoch [400/1000], Loss : 314370176.0000\n",
      "Adam - Epoch [500/1000], Loss : 314369472.0000\n",
      "Adam - Epoch [600/1000], Loss : 314368832.0000\n",
      "Adam - Epoch [700/1000], Loss : 314368160.0000\n",
      "Adam - Epoch [800/1000], Loss : 314367488.0000\n",
      "Adam - Epoch [900/1000], Loss : 314366816.0000\n",
      "Adam - Epoch [1000/1000], Loss : 314366176.0000\n"
     ]
    }
   ],
   "source": [
    "#모델 학습\n",
    "for optimizer_name, optimizer in optimizers.items() :\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for epoch in range(num_epochs) :\n",
    "        inputs = torch.tensor(x_train, dtype=torch.float32)\n",
    "        labels = torch.tensor(y_train, dtype=torch.float32)\n",
    "        \n",
    "        #Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.view_as(outputs))  # 크기를 조절하여 일치시킴\n",
    "        \n",
    "        #Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #Print progress\n",
    "        if (epoch+1) % 100 == 0:\n",
    "            print(f\"{optimizer_name} - Epoch [{epoch+1}/{num_epochs}], Loss : {loss.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
