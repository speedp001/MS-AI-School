{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09239269",
   "metadata": {},
   "source": [
    "<h2 style=\"color: green;\">3. 옵티마이저</h2>\n",
    "\n",
    "옵티마이저(Optimizer)란, 딥러닝 모델의 학습 과정에서 모델의 파라미터를 업데이트하는 알고리즘입니다. 즉, 모델이 학습 데이터에 대해 최적의 결과를 도출하기 위해 모델의 가중치(weight)와 편향(bias)을 조정하는 데 사용됩니다.\n",
    "\n",
    "옵티마이저는 손실 함수(loss function)에서 계산된 그래디언트(gradient)를 이용하여 모델의 파라미터를 업데이트합니다. 손실 함수는 모델의 예측 값과 실제 값의 차이를 계산하는 함수로, 이 값을 최소화하는 방향으로 모델의 파라미터를 조정합니다.\n",
    "\n",
    "**옵티마이저의 종류**\n",
    "\n",
    "+ 경사 하강법(Gradient Descent)\n",
    "<br></br>\n",
    "+ 모멘텀(Momentum)\n",
    "<br></br>\n",
    "+ 아다그라드(Adagrad)\n",
    "<br></br>\n",
    "+ 알엠에스프롭(RMSprop)\n",
    "<br></br>\n",
    "+ 아담(Adam)\n",
    "\n",
    "<br></br>\n",
    "## 경사 하강법 (Gradient Descent)\n",
    "\n",
    "가장 기본적인 최적화 알고리즘 중 하나로, 가중치를 조정할 때 매개변수에 대한 손실함수의 기울기 (gradient)를 이용하여 최적화하는 방법입니다. 즉, 가중치 업데이트는 현재 가중치에서 기울기를 빼는 방식으로 이루어집니다.\n",
    "\n",
    "Gradient Descent는 크게 Batch Gradient Descent, Stochastic Gradient Descent(SGD), Mini-batch Gradient Descent로 나뉩니다.\n",
    "<br></br>\n",
    "\n",
    "+ ### 배치 경사 하강법 *Batch Gradient Descent*\n",
    "\n",
    "Batch Gradient Descent는 전체 데이터셋에 대해 기울기를 계산하고 가중치를 업데이트합니다. 즉, 전체 데이터셋을 한번에 처리하므로 한번에 메모리를 많이 사용하게 되고, 처리시간이 오래 걸릴 수 있습니다. 하지만 전체 데이터셋에 대한 기울기를 계산하므로 최소값에 수렴하는 속도는 빠릅니다.\n",
    "\n",
    "+ ### 미니배치 경사 하강법 *Mini-batch Gradient Descent*\n",
    "\n",
    "Mini-batch Gradient Descent는 전체 데이터셋의 일부(mini-batch)에 대해서만 기울기를 계산하고 가중치를 업데이트합니다. 즉, 전체 데이터셋보다 메모리 사용량이 적으면서, 한 개의 데이터를 처리 하는 SGD보다 안정적으로 최적값에 수렴할 수 있습니다.\n",
    "\n",
    "+ ### 확률적 경사 하강법 *Stochastic Gradient Descent (SGD)*\n",
    "\n",
    "Stochastic Gradient Descent는 데이터 하나씩 기울기를 계산하고 가중치를 업데이트합니다. 즉, 데이터 한 개씩 처리하므로 전체 데이터셋보다 메모리 사용량이 적지만, 최소값에 수렴하는 속도가 느릴 수 있습니다. 또한, 경사 하강 방향이 매번 달라져서 최적값에 수렴하는 과정에서 지그재그로 이동하는 현상이 발생할 수 있습니다.\n",
    "\n",
    "<br></br>\n",
    "## *경사 하강법을 이용한 선형 회귀 모델 구현*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cceab356",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7960e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터, 타깃 데이터 생성, y=2x + 1 관계를 따르는 데이터 생성\n",
    "x = np.array([1,2,3,4,5])\n",
    "y = np.array([3,5,7,9,11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f695cf3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 예측값과 실제값 사이의 오차를 계산하는 함수 정의\n",
    "def compute_error(w0, w1, x, y) :\n",
    "    y_pred = w0 + w1 * x\n",
    "    error = y - y_pred\n",
    "    \n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "53776fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경사 하강법을 수행하는 함수 정의\n",
    "def gradient_descent(x, y, lr=0.05, iterations=1000) :\n",
    "    #n은 데이터 x의 개수\n",
    "    n = len(x)\n",
    "    #w0 -> 절편 값 w1 -> 기울기 값\n",
    "    w0 = 0\n",
    "    w1 = 0\n",
    "    \n",
    "    for i in range(iterations) :\n",
    "        error = compute_error(w0, w1, x, y)\n",
    "        #관행적으로 가중치를 업데이트할 때 -2.0의 상수를 사용\n",
    "        w0 -= lr * (-2.0 / n) * np.sum(error)\n",
    "        w1 -= lr * (-2.0 / n) * np.sum(error * x)\n",
    "        \n",
    "    return w0 , w1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f7cb5d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999836757114 2.0000000045215662\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 경사 하강법을 이용한 선형 회귀 모델 구현 시각화\n",
    "# 모델 학습\n",
    "w0, w1 = gradient_descent(x, y)\n",
    "print(w0, w1)\n",
    "\n",
    "plt.plot(x, w0+w1 * x, color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a595c0",
   "metadata": {},
   "source": [
    "## *다중 선형 : 경사 하강법을 이용한 다중 선형 회귀 구현*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0a3ebb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 생성\n",
    "x1 = np.array([1,2,3,4,5])\n",
    "x2 = np.array([0,1,0,1,0])\n",
    "\n",
    "y = np.array([3,5,7,9,11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d313218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(x1, x2, y, lr, iterations) :\n",
    "    \n",
    "    # 초기값 설정\n",
    "    n = len(y)\n",
    "    beta_0 = 0\n",
    "    beta_1 = 0\n",
    "    beta_2 = 0\n",
    "    \n",
    "    # 경사하강법 수행\n",
    "    for i in range(iterations) :\n",
    "        y_pred = beta_0 + beta_1 * x1 + beta_2 * x2\n",
    "        error = y_pred - y\n",
    "        \n",
    "        # 업데이트\n",
    "        beta_0 -= lr * (1/n) * np.sum(error)\n",
    "        beta_1 -= lr * (1/n) * np.sum(error * x1)\n",
    "        beta_2 -= lr * (1/n) * np.sum(error * x2)\n",
    "        \n",
    "    return beta_0, beta_1, beta_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9edad14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beta_0 >>  0.8907546215358821\n",
      "beta_1 >>  2.0237699271134932\n",
      "beta_2 >>  0.057888309859129156\n"
     ]
    }
   ],
   "source": [
    "beta_0, beta_1, beta_2= gradient_descent(x1, x2, y, 0.01, 1000)\n",
    "print(\"beta_0 >> \", beta_0)\n",
    "print(\"beta_1 >> \", beta_1)\n",
    "print(\"beta_2 >> \", beta_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67cf48ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted y value >>  5.996182785621997\n"
     ]
    }
   ],
   "source": [
    "x1_new = 2\n",
    "x2_new = 1\n",
    "\n",
    "# 예측 되어야하는 값 5\n",
    "y_pred_temp = beta_0 + beta_1 * x1_new + beta_2 + x2_new\n",
    "print(\"Predicted y value >> \", y_pred_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d7a21d",
   "metadata": {},
   "source": [
    "## 모멘텀 (Momentum)\n",
    "\n",
    "+ ### *Momentum Optimization*\n",
    "\n",
    "Momentum Optimization은 Gradient Descent의 한계를 보완하기 위해 등장한 방법 중 하나입니다. 이전 기울기의 방향과 크기를 고려하여 새로운 기울기를 계산합니다. 이전 기울기의 방향이 현재 기울기와 일치하면 가중치를 더 크게 업데이트하고, 그렇지 않으면 더 작게 업데이트합니다.\n",
    "\n",
    "예를 들어, 기울기가 대각선 방향으로 지속적으로 나오는 경우, 일반적인 Gradient Descent는 오랜 시간 동안 최적점에 수렴하지 못하고 지그재그로 움직이게 됩니다. 하지만 Momentum Optimization은 이전 기울기를 더해서 가중치 업데이트를 수행하기 때문에 이전 방향을 유지하면서 최적점에 빠르게 수렴할 수 있습니다.\n",
    "\n",
    "이전 기울기와 현재 기울기의 비율을 조절하는 momentum 하이퍼파라미터를 설정할 수 있습니다. momentum 값이 0에 가까울수록 Gradient Descent와 유사해지며, 1에 가까울수록 이전 방향을 보존하는 정도가 높아집니다. 적절한 momentum 값을 설정하면 보다 빠르고 안정적인 학습을 할 수 있습니다.\n",
    "\n",
    "<br></br>\n",
    "## 아다그라드 (Adagrad)\n",
    "\n",
    "+ ### *Adagrad Optimization*\n",
    "\n",
    "각각의 매개변수에 서로 다른 학습률을 적용하는 방식을 사용하여, 데이터 셋에서 매개변수에 대한\n",
    "제곱된 그래디언트의 역사를 누적함으로써 각각의 매개변수에 대한 적응적인 학습률을 계산합니다.\n",
    "기울기 제곱 값의 누적 합을 이용해 학습률을 조절하는 방식으로, 매개변수별로 학습률을 조절할 수 있는 방법입니다. 이전 기울기들의 제곱을 누적하여 학습률을 조절하므로 처음에는 크게 업데이트하다가 점차 학습률이 줄어들게 됩니다. 이러한 방식은 기울기의 크기가 큰 매개변수는 학습률이 감소하고, 기울기의 크기가 작은 매개변수는 학습률이 증가하는 경향을 보입니다.\n",
    "\n",
    "Adagrad의 장점으로는 매개변수의 전역 학습률을 조절하는 것이 아니라, 개별 매개변수의 학습률을 적절하게 조절할 수 있어, 매개변수의 스케일에 덜 민감해진다는 점이 있습니다. 하지만, 단점으로는 누적된 제곱 기울기 값이 계속해서 커져서 학습률이 점점 작아지는 문제가 있어, 학습이 오래될수록 업데이트가 매우 느려지는 경향이 있습니다. 이러한 단점을 보완하기 위해 RMSprop 과 Adam이 등장하게 됩니다.\n",
    "\n",
    "Adagrad 최적화 기법에는 다른 종류가 없습니다. Adagrad는 하이퍼파라미터인 학습률(learning rate)을 매개변수마다 따로 설정하는 방식으로 동작하므로, 하나의 하이퍼파라미터만 설정하면 됩니다.\n",
    "\n",
    "<br></br>\n",
    "## 알엠에스프롭 (RMSprop)\n",
    "\n",
    "+ ### *RMSprop Optimization*\n",
    "\n",
    "Adagrad의 단점을 보완한 방법 중 하나로, 기울기 제곱의 이동평균을 사용하여 학습률을 조절합니다.\n",
    "\n",
    "Adagrad는 학습률을 조절하는 방법으로 각 매개변수에 대한 학습률을 따로 설정하였습니다. 이 때, 빈번하게 발생하는 기울기의 작은 값으로 인해 학습률이 지나치게 작아지는 문제가 발생합니다. RMSprop 은 이 문제를 해결하기 위해 기울기 제곱의 이동평균을 사용합니다.\n",
    "\n",
    "RMSprop은 이동평균을 구할 때 지수이동평균(Exponential Moving Average, EMA)을 사용합니다. EMA는 이동평균을 구할 때 과거의 값들을 지수적으로 감소시키면서 현재 값을 계산하는 방식입니다. 이 때, 이동평균을 구하는 지수가중치(decay rate)를 하이퍼파라미터로 설정할 수 있습니다.\n",
    "\n",
    "**RMSprop의 학습률 조절 방법은 다음과 같습니다.**\n",
    "1. 기울기 제곱의 이동평균을 구합니다.\n",
    "2. 구한 이동평균으로 학습률을 조절합니다.\n",
    "\n",
    "즉, RMSprop은 과거 기울기의 크기를 고려하여 적절한 학습률을 계산합니다. 이러한 방식으로 RMSprop은 Adagrad보다 빠르게 수렴하면서도 더 안정적으로 학습할 수 있습니다.\n",
    "\n",
    "<br></br>\n",
    "## 아담 (Adam)\n",
    "\n",
    "Momentum Optimization과 Adagrad Optimization의 아이디어를 결합한 옵티마이저입니다. Adam은 각 매개 변수마다 적응적인 학습률을 사용하며, 이전 기울기의 지수 가중 이동 평균과 이전 기울기 제곱의 지수 가중 이동 평균을 계산하여 학습률을 조정합니다.\n",
    "\n",
    "**Adam은 다음과 같은 식을 사용합니다.**\n",
    "1. 현재 시점의 기울기를 계산합니다.\n",
    "2. 이전 기울기의 지수 가중 이동평균과 이전 기울기 제곱의 지수 가중 이동평균을 계산합니다.\n",
    "3. 학습률을 적응적으로 조정합니다.\n",
    "4. 매개변수를 업데이트합니다.\n",
    "\n",
    "Momentum Optimization과 마찬가지로 이전 기울기의 방향과 크기를 고려하면서 매개변수를 업데이트 하기 때문에, 경사면이 급격하게 변하는 지점에서 빠르게 최적점에 다가갈 수 있습니다. 또한, Adagrad Optimization처럼 각 매개변수마다 적응적인 학습률을 사용하기 때문에, 학습이 더욱 안정적으로 이루어질 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5961c2f2",
   "metadata": {},
   "source": [
    "## *선형 회귀 모델의 학습에서 다양한 옵티마이저를 적용*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "039656f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0d4101f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "california = fetch_california_housing()\n",
    "x = california.data\n",
    "y = california.target\n",
    "\n",
    "# 데이터 스케일링\n",
    "scaler = StandardScaler()\n",
    "x = scaler.fit_transform(x)\n",
    "# 데이터를 표준화(standardization)합니다. 표준화는 각 특징의 평균을 0, 표준 편차를 1로 만들어 데이터의 스케일을 일정하게 조정하는 과정입니다.\n",
    "\n",
    "# 데이터 분할\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 모델 생성 및 하이퍼파라미터 설정\n",
    "input_dim = x.shape[1]\n",
    "output_dim = 1\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 1000\n",
    "\n",
    "model = nn.Linear(input_dim, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b977c8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다양한 옵티마이저 설정\n",
    "optimizers = {\"SGD\" : optim.SGD(model.parameters(), lr=learning_rate),\n",
    "             \"Momentum\" : optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9),\n",
    "             \"Adagrad\" : optim.Adagrad(model.parameters(), lr=learning_rate),\n",
    "             \"RMSprop\" : optim.RMSprop(model.parameters(), lr=learning_rate),\n",
    "             \"Adam\" : optim.Adam(model.parameters(), lr=learning_rate)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "00ca5c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD - Epoch [100/1000], Loss : 0.7637\n",
      "SGD - Epoch [200/1000], Loss : 5.3000\n",
      "SGD - Epoch [300/1000], Loss : 1.8422\n",
      "SGD - Epoch [400/1000], Loss : 3.8251\n",
      "SGD - Epoch [500/1000], Loss : 3.3539\n",
      "SGD - Epoch [600/1000], Loss : 2.4609\n",
      "SGD - Epoch [700/1000], Loss : 4.5974\n",
      "SGD - Epoch [800/1000], Loss : 1.5708\n",
      "SGD - Epoch [900/1000], Loss : 5.2728\n",
      "SGD - Epoch [1000/1000], Loss : 1.0989\n",
      "Momentum - Epoch [100/1000], Loss : 1.2320\n",
      "Momentum - Epoch [200/1000], Loss : 7.1784\n",
      "Momentum - Epoch [300/1000], Loss : 238.3185\n",
      "Momentum - Epoch [400/1000], Loss : 1181.8108\n",
      "Momentum - Epoch [500/1000], Loss : 425.5255\n",
      "Momentum - Epoch [600/1000], Loss : 132312.6719\n",
      "Momentum - Epoch [700/1000], Loss : 1331828.2500\n",
      "Momentum - Epoch [800/1000], Loss : 771723.1875\n",
      "Momentum - Epoch [900/1000], Loss : 54261868.0000\n",
      "Momentum - Epoch [1000/1000], Loss : 1145958784.0000\n",
      "Adagrad - Epoch [100/1000], Loss : 1176655872.0000\n",
      "Adagrad - Epoch [200/1000], Loss : 1176655872.0000\n",
      "Adagrad - Epoch [300/1000], Loss : 1176655872.0000\n",
      "Adagrad - Epoch [400/1000], Loss : 1176655872.0000\n",
      "Adagrad - Epoch [500/1000], Loss : 1176655744.0000\n",
      "Adagrad - Epoch [600/1000], Loss : 1176655744.0000\n",
      "Adagrad - Epoch [700/1000], Loss : 1176655744.0000\n",
      "Adagrad - Epoch [800/1000], Loss : 1176655872.0000\n",
      "Adagrad - Epoch [900/1000], Loss : 1176655872.0000\n",
      "Adagrad - Epoch [1000/1000], Loss : 1176655744.0000\n",
      "RMSprop - Epoch [100/1000], Loss : 1176651392.0000\n",
      "RMSprop - Epoch [200/1000], Loss : 1176651392.0000\n",
      "RMSprop - Epoch [300/1000], Loss : 1176651136.0000\n",
      "RMSprop - Epoch [400/1000], Loss : 1176651136.0000\n",
      "RMSprop - Epoch [500/1000], Loss : 1176651008.0000\n",
      "RMSprop - Epoch [600/1000], Loss : 1176651008.0000\n",
      "RMSprop - Epoch [700/1000], Loss : 1176651008.0000\n",
      "RMSprop - Epoch [800/1000], Loss : 1176650752.0000\n",
      "RMSprop - Epoch [900/1000], Loss : 1176650752.0000\n",
      "RMSprop - Epoch [1000/1000], Loss : 1176650752.0000\n",
      "Adam - Epoch [100/1000], Loss : 1176650496.0000\n",
      "Adam - Epoch [200/1000], Loss : 1176650496.0000\n",
      "Adam - Epoch [300/1000], Loss : 1176650496.0000\n",
      "Adam - Epoch [400/1000], Loss : 1176650368.0000\n",
      "Adam - Epoch [500/1000], Loss : 1176650368.0000\n",
      "Adam - Epoch [600/1000], Loss : 1176649984.0000\n",
      "Adam - Epoch [700/1000], Loss : 1176649984.0000\n",
      "Adam - Epoch [800/1000], Loss : 1176649984.0000\n",
      "Adam - Epoch [900/1000], Loss : 1176649856.0000\n",
      "Adam - Epoch [1000/1000], Loss : 1176649856.0000\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "for optimizer_name, optimizer in optimizers.items() :\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for epoch in range(num_epochs) :\n",
    "        inputs = torch.tensor(x_train, dtype=torch.float32)\n",
    "        labels = torch.tensor(y_train, dtype=torch.float32)\n",
    "        \n",
    "        #Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels.view_as(outputs))  # 크기를 조절하여 일치시킴\n",
    "        \n",
    "        #Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #Print progress\n",
    "        if (epoch+1) % 100 == 0:\n",
    "            print(f\"{optimizer_name} - Epoch [{epoch+1}/{num_epochs}], Loss : {loss.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
