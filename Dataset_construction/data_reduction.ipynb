{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "856861f6",
   "metadata": {},
   "source": [
    "<h1 style=\"color: blue;\">1. 다양한 데이터 처리 방법</h1>\n",
    "\n",
    "데이터를 생성해서 무사히 저장하기까지 일련의 과정을 데이터 파이프라인이라고 합니다.\n",
    "\n",
    "1. 데이터 생성\n",
    "2. 데이터 수집\n",
    "3. 데이터 가공 후 저장(ETL : 데이터 추출, 변환, 적재를 묶어서 ETL이라고 부릅니다. )\n",
    "4. 데이터 시각화(BI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cae08a",
   "metadata": {},
   "source": [
    "<h2 style=\"color: green;\">데이터프레임과 데이터레이크</h2>\n",
    "\n",
    "+ 데이터 웨어하우스(data warehouse) : 사용자의 의사 결정에 도움을 주기 위하여 기간시스템의 데이터 베이스에 축적된 데이터를 공통의 형식으로 변환해서 관리하는 데이터베이스를 말합니다. 줄여서 DW로도 불립니다.\n",
    "\n",
    "    즉 데이터 웨어하우스는 방대한 조직 내에서 분산 운영되는 각각의 데이터 베이스 관리 시스템들을 효 율적으로 통합하여 조정 관리 하는 것을 의미합니다.\n",
    "<br></br>\n",
    "+ 데이터 레이크 : 구조화되거나 반구조화되거나 구조화되지 않은 대량의 데이터를 저장, 처리, 보호하기 위한 중앙 집중식 저장소입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a216fa80",
   "metadata": {},
   "source": [
    "<h2 style=\"color: green;\">데이터분석 프로세스</h2>\n",
    "\n",
    "#### 문제 정의 -> 데이터 수집 -> 데이터 전처리 -> 모델링 -> 해석 및 시각화\n",
    "\n",
    "#### 1. 문제 정의\n",
    "\n",
    "데이터 분석 프로세스에서 가장 중요한 단계이자 어려운 단계 문제가 제대로 설정되지 않는다면 다시 처음부터 진행하므로 시간적 손실 발생\n",
    "\n",
    "- 분석의대상,분석목적설정\n",
    "- 정의된 문제를 해결하기 위한 구체적인 계획이 수립\n",
    "- 문제를 정의할 분야에 대한 비즈니스 지식이 필요\n",
    "- 모든 사람들이 명료하게 이해할 수 있도록 구체적\n",
    "\n",
    "#### 2. 데이터 수집\n",
    "\n",
    "분석에필요한데이터를찾고모으는단계데이터의양이많다고좋은것은아니다.\n",
    "\n",
    "- 데이터수집단계가어려운이유는데이터가존재하지않는경우혹은너무많은경우\n",
    "- 비용과법적인문제가발생할수있으니주의필요\n",
    "- 최근 데이터 수집 방법에는 데이터 구매, 실험환경에서 수집(스스로 데이터 생성), 웹 크롤링, open dataset을 활용하여 수집한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2fb054",
   "metadata": {},
   "source": [
    "#### 3. 데이터 전처리\n",
    "\n",
    "수집한 데이터를 바로 분석에 적용하는 경우는 거의 없다. 전처리가 필요하다. 왜냐하면 데이터를 생\n",
    "생할 때 분석을 전제로 데이터를 생성하지 않았기 때문이다.\n",
    "\n",
    "<데이터 전처리>\n",
    "\n",
    "- 분석에 부적한 구조, 누락된 항목, NA(결측값) 존재 등으로 인해 전처리 과정이 필요하다.\n",
    "- 노이즈 제거, 중복값 제거, 결측값 보정, 데이터 연계/통합, 데이터 구조 변경\n",
    "- 데이터 백터화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fc52c6",
   "metadata": {},
   "source": [
    "#### 4. 모델링\n",
    "\n",
    "원하는 결과를 도출하기 위해서 예측이든 분류, 희귀를 위한 작업을 진행하는 단계이다. 즉 전처리된\n",
    "데이터를 활용하여 학습을 진행하는 단계\n",
    "\n",
    "#### 5. 해석 및 시각화\n",
    "\n",
    "모델링을 통해 결과를 도출되면 이를 처음 정의했던 문제와 연관시켜 문제를 해결하는 방법을 모색\n",
    "하는 단계. 시각화를 통해 도출된 결과를 알아보기 쉽게 표현하고, 이를 근거로 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3c5cf5",
   "metadata": {},
   "source": [
    "<h2 style=\"color: green;\">정형, 비정형, 반정형</h2>\n",
    "\n",
    "정형 데이터 : 데이터베이스의 사전에 정의된 규칙에 맞게 들어간 데이터 중에 수치 만으로 의미 파악이 쉬운 데이터를 의미합니다.\n",
    "\n",
    "비정형 데이터 : 비정형 데이터는 정형 데이터와 반대되는 단어이다. 비정형 데이터는 형태가 없고, 연산이 불가능한 데이터를 의미한다. 정해진 규칙이 없기 때문에 데이터의 의미를 쉽게 파악하기 힘들다.\n",
    "\n",
    "반정형 데이터 (Semi-structed data) :반정형 데이터의 반은 Semi를 의미합니다. 즉 완전한 정형이 아닌 약한 정형 데이터이다. 데이터베이스는 아니지만 스키마를 가지고 있는 형태이고, 연산이 불가능한 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48894472",
   "metadata": {},
   "source": [
    "<h2 style=\"color: green;\">데이터 축소</h2>\n",
    "\n",
    "적은 양으로도 전체 데이터 집합을 잘 대표하는 데이터 얻는 과정이다. 대규모 데이터의 작업 시 분석에 필요한 줄이고 효율성을 향상시키기 위해 필요하다.\n",
    "\n",
    "### 특성 추출을 사용한 차원 축소\n",
    "\n",
    "+ 차원 축소를 위한 특성 추출의 목적은 특성에 내재된 정보는 많이 유지하면서 특성 집합 Poriginal 을 새로운 집합 Pnew 으로 변환하는 것입니다.\n",
    "+ 고품질예측을만들기위한데이터의능력을조금만희생하고특성의수를줄입니다.\n",
    "+ 특성추출기법의단점은새로운특성을사람이이해하지못한다는것입니다.\n",
    "+ 모델을훈련하기위해필요한특성을담고있지만사람의눈에는무작위한숫자의모음으로보일것입니다.\n",
    "\n",
    "### 주성분을 사용해 특성 줄이기\n",
    "\n",
    "+ PCA는데이터의분산을유지하면서특성의수를줄이는선형차원축소기법입니다\n",
    "+ PCA는타킷벡터의정보를사용하지않고특성행렬만이용하는비지도학습기법입니다\n",
    "+ 데이터는 두개의 특성 x1과 x2를 가집니다\n",
    "+ 그래프의 샘플들은 길이가 길고 높이는 낮은 타원 모양으로 퍼져 있는 '길이' 방향의 분산이 '높이'방향보다 훨씬 큽니다.\n",
    "+ 길이와높이대신가장분산이많은방향을첫번째주성분으로부르고두번째로가장많은방향을두번째주 성분이라고 부릅니다.\n",
    "+ 특성을줄이는한가지방법은2D공간의모든샘플을1차원주성분에투영하는것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ee335c",
   "metadata": {},
   "source": [
    "### *사이킷런 손글씨 데이터를 활용하여 특성 행렬을 표준화 처리*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "557abca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69f2393f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.         -0.33501649 -0.04308102 ... -1.14664746 -0.5056698\n",
      "  -0.19600752]\n",
      " [ 0.         -0.33501649 -1.09493684 ...  0.54856067 -0.5056698\n",
      "  -0.19600752]\n",
      " [ 0.         -0.33501649 -1.09493684 ...  1.56568555  1.6951369\n",
      "  -0.19600752]\n",
      " ...\n",
      " [ 0.         -0.33501649 -0.88456568 ... -0.12952258 -0.5056698\n",
      "  -0.19600752]\n",
      " [ 0.         -0.33501649 -0.67419451 ...  0.8876023  -0.5056698\n",
      "  -0.19600752]\n",
      " [ 0.         -0.33501649  1.00877481 ...  0.8876023  -0.26113572\n",
      "  -0.19600752]]\n"
     ]
    }
   ],
   "source": [
    "digits = datasets.load_digits()  #8x8 크기의 손글씨 숫자 데이터 로드\n",
    "feature = StandardScaler().fit_transform(digits.data)  #특성 행렬을 표준화 처리\n",
    "\n",
    "print(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c0fb70",
   "metadata": {},
   "source": [
    "1. StandardScaler() \n",
    ">> Scikit-learn의 전처리(preprocessing) 모듈에서 제공되는 클래스 중 하나입니다. 이 클래스는 데이터를 평균이 0, 분산이 1인 가우시안 정규 분포(standard normal distribution)로 변환합니다.\n",
    "\n",
    " \n",
    "\n",
    "2. digits.data : digits 데이터셋에서 숫자 이미지의 각 픽셀 값을 포함하는 배열\n",
    "<br></br>\n",
    "3. fit_transform() 메서드\n",
    ">> StandardScaler 클래스에는 데이터를 변환하는 두 가지 단계가 있습니다. 첫째, 모델을 학습(fit)하고, 둘째, 학습된 모델을 사용하여 데이터를 변환(transform)합니다. fit_transform() 메서드는 이 두 단계를 한 번에 수행합니다. 즉, 데이터를 표준화(normalize)하고, 변환된 값을 반환합니다.\n",
    "\n",
    "따라서 위의 코드는 digits 데이터셋의 특성을 가우시안 정규 분포로 변환한 후, 변환된 값을 featuress 변수에 할당 이렇게 정규화를 수행하면, 모델이 데이터를 더 잘 이해하고, 모델의 예측 성능을 향상 시킬 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ca5923",
   "metadata": {},
   "source": [
    "### *99%의 분산을 유지하도록 PCA 클래스 객체 생성*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a10e33ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7f53c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 특성 개수 >>  64\n",
      "줄어든 특성 개수 >>  54\n"
     ]
    }
   ],
   "source": [
    "pca = PCA(n_components = 0.99, whiten=True)\n",
    "features_pca = pca.fit_transform(feature)  #PCA 수행\n",
    "\n",
    "print(\"원본 특성 개수 >> \", feature.shape[1])\n",
    "print(\"줄어든 특성 개수 >> \", features_pca.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea008d69",
   "metadata": {},
   "source": [
    "PCA 클래스: Scikit-learn의 decomposition 모듈에서 제공되는 클래스 중 하나입니다. PCA는 데이터셋의 차원을 감소시키는 기술로, 데이터셋에서 가장 중요한 특성만 추출하여 새로운 차원 축으로 변환합니다. 이를 통해 데이터셋의 노이즈(noise)를 제거하고, 더욱 빠르고 효율적인 학습이 가능해집니다.\n",
    "\n",
    "n_components: PCA 클래스의 인자 중 하나로, 추출할 주성분(principal component)의 수를 지정합니다. 여기서는 99%의 분산(variance)을 유지하도록 설정되어 있습니다. 이는 데이터셋에서 99%의 정보가 유지되도록 차원을 축소하는 것을 의미합니다.\n",
    "\n",
    "whiten: PCA 클래스의 인자 중 하나로, True로 설정할 경우 PCA의 결과로 나오는 주성분들이 서로 독립적인 값이 되도록 백색화(whitening)를 수행합니다. 백색화를 하면 각 주성분의 분산이 1이 되고, 상관 관계가 없는 성분들로 구성된 새로운 특성 공간이 만들어집니다.\n",
    "\n",
    "위의 같이 PCA이용하면 99%의 분산을 유지하도록 새로운 특성(feature) 공간으로 변환하고 있습니다. 결과적으로, 원본 데이터셋의 특성 개수는 features.shape[1]으로 확인할 수 있고, PCA를 수행하여 감소된 특성 개수는 features_pca.shape[1]으로 확인할 수 있습니다. 이렇게 차원 축소를 수행하면, 모델의 학습 시간을 단축시키고, 과적합(overfitting)을 방지할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57843f3",
   "metadata": {},
   "source": [
    "### *사이킷 런 손글씨 데이터를 활용하여 주성분 줄이기 차이(결과값 비교)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4c4bfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11448971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1617 180\n",
      "1617 180\n"
     ]
    }
   ],
   "source": [
    "digits = load_digits()\n",
    "\n",
    "#진짜 차이가 있는지 체크 하기 위해서 -> 정규화 하지 않은 데이터로 분류 모델 훈련\n",
    "x_train, x_test, y_train, y_test = train_test_split(digits.data, digits.target,test_size=0.1, random_state=777)\n",
    "\n",
    "print(len(x_train), len(x_test))\n",
    "print(len(y_train), len(y_test))\n",
    "\n",
    "# 8 : 2\n",
    "# 2 -> 1 : 1\n",
    "# 8 : 1 : 1\n",
    "\n",
    "# 1.폴더 생성 (이미지 100개)\n",
    "# 2.폴더 읽고 -> train val test 나눠서 폴더 생성해서 거기에 이미지 저장 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b535cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9444444444444444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sang-yun/anaconda3/envs/AI/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#모델 불러오기\n",
    "model = LogisticRegression()\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "no_standardScaler_acc_score = accuracy_score(y_test, y_pred)  #정답지 예측치\n",
    "print(no_standardScaler_acc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132d9560",
   "metadata": {},
   "source": [
    "### *StandardScaler 적용 후 -> 평가*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e862ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9444444444444444\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "x_train_norm = scaler.fit_transform(x_train)\n",
    "x_test_norm = scaler.transform(x_test)\n",
    "model_norm = LogisticRegression()\n",
    "model_norm.fit(x_train_norm, y_train)\n",
    "y_pred_norm = model_norm.predict(x_test_norm)\n",
    "\n",
    "standardScale_acc_score = accuracy_score(y_test, y_pred_norm)\n",
    "print(standardScale_acc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379b4b0d",
   "metadata": {},
   "source": [
    "### *선형적으로 구분되지 않는 데이터의 차원 축소*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64edfd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.datasets import make_circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "482bbbf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.02313275  0.15282194]\n",
      " [ 0.8626983   0.46556009]\n",
      " [ 0.05905661 -0.05048386]\n",
      " ...\n",
      " [-0.10071513 -0.1814104 ]\n",
      " [ 0.10461224  0.08710802]\n",
      " [ 0.28894733  0.97460534]]\n"
     ]
    }
   ],
   "source": [
    "#선형적으로 구분되지 않은 데이터를 만들기\n",
    "feature, _ = make_circles(n_samples=1000, random_state=777, noise=0.1, factor=0.1)\n",
    "#make_circles는 feature과 label 두개의 값이 반환된다.\n",
    "print(feature)\n",
    "\n",
    "#make_circles는 원모양의 데이터셋을 생성하므로서 비선형 데이터셋이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b782f254",
   "metadata": {},
   "source": [
    "### *방사 기저 함수를 사용하여 커널 PCA를 적용*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "75e61051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 특성 개수 :  2\n",
      "줄어든 특성 개수 :  1\n"
     ]
    }
   ],
   "source": [
    "kcpa = KernelPCA(kernel = 'rbf', gamma=15, n_components=1)\n",
    "#kcpa rbf\n",
    "feature_rbf = kcpa.fit_transform(feature)\n",
    "print(\"원본 특성 개수 : \", feature.shape[1])\n",
    "print(\"줄어든 특성 개수 : \", feature_rbf.shape[1])\n",
    "#.shape[]은 튜플형태로 반환 [0]은 행의 개수 [1]은 열의 개수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e52e74",
   "metadata": {},
   "source": [
    "### *선형 판별 분석 LDA 실습*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e9e3a249",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e8c852be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.4 3.7 1.5 0.2]\n",
      " [4.8 3.4 1.6 0.2]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [4.3 3.  1.1 0.1]\n",
      " [5.8 4.  1.2 0.2]\n",
      " [5.7 4.4 1.5 0.4]\n",
      " [5.4 3.9 1.3 0.4]\n",
      " [5.1 3.5 1.4 0.3]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [5.4 3.4 1.7 0.2]\n",
      " [5.1 3.7 1.5 0.4]\n",
      " [4.6 3.6 1.  0.2]\n",
      " [5.1 3.3 1.7 0.5]\n",
      " [4.8 3.4 1.9 0.2]\n",
      " [5.  3.  1.6 0.2]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [5.2 3.5 1.5 0.2]\n",
      " [5.2 3.4 1.4 0.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [4.8 3.1 1.6 0.2]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.2 4.1 1.5 0.1]\n",
      " [5.5 4.2 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.2]\n",
      " [5.  3.2 1.2 0.2]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.6 1.4 0.1]\n",
      " [4.4 3.  1.3 0.2]\n",
      " [5.1 3.4 1.5 0.2]\n",
      " [5.  3.5 1.3 0.3]\n",
      " [4.5 2.3 1.3 0.3]\n",
      " [4.4 3.2 1.3 0.2]\n",
      " [5.  3.5 1.6 0.6]\n",
      " [5.1 3.8 1.9 0.4]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [5.1 3.8 1.6 0.2]\n",
      " [4.6 3.2 1.4 0.2]\n",
      " [5.3 3.7 1.5 0.2]\n",
      " [5.  3.3 1.4 0.2]\n",
      " [7.  3.2 4.7 1.4]\n",
      " [6.4 3.2 4.5 1.5]\n",
      " [6.9 3.1 4.9 1.5]\n",
      " [5.5 2.3 4.  1.3]\n",
      " [6.5 2.8 4.6 1.5]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [4.9 2.4 3.3 1. ]\n",
      " [6.6 2.9 4.6 1.3]\n",
      " [5.2 2.7 3.9 1.4]\n",
      " [5.  2.  3.5 1. ]\n",
      " [5.9 3.  4.2 1.5]\n",
      " [6.  2.2 4.  1. ]\n",
      " [6.1 2.9 4.7 1.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.7 3.1 4.4 1.4]\n",
      " [5.6 3.  4.5 1.5]\n",
      " [5.8 2.7 4.1 1. ]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.9 3.2 4.8 1.8]\n",
      " [6.1 2.8 4.  1.3]\n",
      " [6.3 2.5 4.9 1.5]\n",
      " [6.1 2.8 4.7 1.2]\n",
      " [6.4 2.9 4.3 1.3]\n",
      " [6.6 3.  4.4 1.4]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [6.7 3.  5.  1.7]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [5.7 2.6 3.5 1. ]\n",
      " [5.5 2.4 3.8 1.1]\n",
      " [5.5 2.4 3.7 1. ]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.  2.7 5.1 1.6]\n",
      " [5.4 3.  4.5 1.5]\n",
      " [6.  3.4 4.5 1.6]\n",
      " [6.7 3.1 4.7 1.5]\n",
      " [6.3 2.3 4.4 1.3]\n",
      " [5.6 3.  4.1 1.3]\n",
      " [5.5 2.5 4.  1.3]\n",
      " [5.5 2.6 4.4 1.2]\n",
      " [6.1 3.  4.6 1.4]\n",
      " [5.8 2.6 4.  1.2]\n",
      " [5.  2.3 3.3 1. ]\n",
      " [5.6 2.7 4.2 1.3]\n",
      " [5.7 3.  4.2 1.2]\n",
      " [5.7 2.9 4.2 1.3]\n",
      " [6.2 2.9 4.3 1.3]\n",
      " [5.1 2.5 3.  1.1]\n",
      " [5.7 2.8 4.1 1.3]\n",
      " [6.3 3.3 6.  2.5]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [7.1 3.  5.9 2.1]\n",
      " [6.3 2.9 5.6 1.8]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [7.6 3.  6.6 2.1]\n",
      " [4.9 2.5 4.5 1.7]\n",
      " [7.3 2.9 6.3 1.8]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [7.2 3.6 6.1 2.5]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [6.4 2.7 5.3 1.9]\n",
      " [6.8 3.  5.5 2.1]\n",
      " [5.7 2.5 5.  2. ]\n",
      " [5.8 2.8 5.1 2.4]\n",
      " [6.4 3.2 5.3 2.3]\n",
      " [6.5 3.  5.5 1.8]\n",
      " [7.7 3.8 6.7 2.2]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.2 5.  1.5]\n",
      " [6.9 3.2 5.7 2.3]\n",
      " [5.6 2.8 4.9 2. ]\n",
      " [7.7 2.8 6.7 2. ]\n",
      " [6.3 2.7 4.9 1.8]\n",
      " [6.7 3.3 5.7 2.1]\n",
      " [7.2 3.2 6.  1.8]\n",
      " [6.2 2.8 4.8 1.8]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.2 3.  5.8 1.6]\n",
      " [7.4 2.8 6.1 1.9]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [6.3 2.8 5.1 1.5]\n",
      " [6.1 2.6 5.6 1.4]\n",
      " [7.7 3.  6.1 2.3]\n",
      " [6.3 3.4 5.6 2.4]\n",
      " [6.4 3.1 5.5 1.8]\n",
      " [6.  3.  4.8 1.8]\n",
      " [6.9 3.1 5.4 2.1]\n",
      " [6.7 3.1 5.6 2.4]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [5.8 2.7 5.1 1.9]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [6.7 3.3 5.7 2.5]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.3 2.5 5.  1.9]\n",
      " [6.5 3.  5.2 2. ]\n",
      " [6.2 3.4 5.4 2.3]\n",
      " [5.9 3.  5.1 1.8]] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "iris = datasets.load_iris()  #붓꽃 데이터셋 로드\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "print(features, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0622d213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 특성 개수 >>  4\n",
      "줄어든 특성 개수 >>  1\n"
     ]
    }
   ],
   "source": [
    "lda = LinearDiscriminantAnalysis(n_components=1)\n",
    "feature_lda = lda.fit(features, target).transform(features)\n",
    "\n",
    "print(\"원본 특성 개수 >> \", features.shape[1])\n",
    "print(\"줄어든 특성 개수 >> \", feature_lda.shape[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
